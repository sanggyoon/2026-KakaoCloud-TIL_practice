## 목차

1. 딥러닝 기초

---

## 1) 딥러닝 기초

- **신경망 구조**
  - 입력층: 데이터가 들어오는 층, 텍스트라면 단어 벡터들이 입력되는 층
  - 은닉층: 학습과 특징 추출이 일어나는 곳, 은닉층이 여러개면 딥러닝이라 불림
  - 출력층: 결과가 출력되는 층, 분류 문제면 클래스와 확률을, 회귀 문제라면 예측 값 출력
- **가중치(W), 편향(b)**
  - 가중치 Weight: 어떤 입력이 더 중요한지 결정, 중요할 수록 높아짐
    - W1: 입력층에서 은닉층으로의 연결 강도
    - W2: 은닉층에서 출력층으로의 연결 강도
  - 편향 bias: 활성화 함수의 임계값을 조절하는 역할, 모든 입력이 0이어도 뉴런이 활성화될 수 있게 함.
- **활성화 함수(Activation Function)**
  - 가중 합계를 다음 층으로 보낼지 결정하며 비선형성을 부여
  - ReLU: 0보다 작으면 0, 크면 그대로 출력, 가장 많이 사용됨.
  - Sigmoid: 입력값을 0과 1 사이 값으로 변환, 이진 분류 문제에 많이 사용, 기술기 소실 문제 있음.
  - Softmax: 다중 분류 문제의 출력층에서 사용, 각 클래스의 확률의 모든 합이 1이 됨
- **손실 함수**
  - 평균 제곱 오차(MSE): 회귀 문제
  - 교차 엔트로피:(Cross-Entropy): 분류 문제
  - 딥러닝 모델이 얼마나 틀렸는지 측정하는 함수
    - L=Loss( y^ ,y)
    - L=Loss(f(X;θ),y) (여기서 θ는 모든 가중치와 편향)
- **옵티마이저(Optimizer), 역전파(Backpropagation)**
  - 옵티마이저(Optimizer): 신경망의 가중치를 어떻게 업데이트할지 결정하는 알고리즘. SGD, Adam, RMSprop, 경사하강법 등이 있음.
  - 역전파(Backpropagation): 출력층에서 계산된 오차를 입력층 방향으로 거슬러 올라가며 각 가중치가 오차에 기여한 정도를 계산하는 방법(미분의 연쇄 법칙)
    - 오차 계산: loss = (예측 - 정답)²
    - 연쇄법칙: ∂loss/∂W = ∂loss/∂a × ∂a/∂z × ∂z/∂W
- **수식화**
  $$
  y = f(W*x + b)
  $$
- **학습 사이클**
  - 순전파: 입력 → 신경망 → 예측값
  - 손실 계산: 손실 = (예측값 - 정답)^2
  - 역전파: 손실을 줄이기 위한 가중치 조정
  - 가중치 업데이트: W = W - 학습률 \* 기울기
- **연쇄 법칙**
  - 합성함수의 미분을 구하는 수학 규칙
  - 딥러닝의 역전파 알고리즘의 핵심 원리
  - 복잡한 함수를 단순한 부분으로 나누어 미분 가능
  - 오차 전파 과정을 단계별로 계산
